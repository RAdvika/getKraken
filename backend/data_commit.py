import json
import os.path
import kagglehub
import pandas as pd
import sklearn as sk
import re

from dotenv import dotenv_values
from github import Github
from github import Auth

env = dotenv_values('./env/.env')

# download latest version
path = kagglehub.dataset_download("nikhil25803/github-dataset")
path = os.path.join(path, 'github_dataset.csv')

print("Path to dataset files:", path)

df = pd.read_csv(path)
df = df.dropna()  # drop all rows with NaN values
df = df[df['stars_count'] > 100]  # drop all repos with less than 100 stars
df = df[df['issues_count'] > 25]  # drop all repos with less than 25 issues open
df = df.drop_duplicates()  # drop dupes

filter = [
    'octocat/octocat.github.io',
    'EddieHubCommunity/awesome-github-profiles',
    'education/classroom-assistant',
    'education/teachers_pet'
]

df = df[df['repositories'].isin(filter) == False]

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(df)
    print(df.shape)

try:
    with open('sample_data.json', 'r') as f:
        dataset = json.loads(f.read())

    bag = []
    for v in dataset.values():
        bag.extend(v['issues'])
        bag.extend(v['commits'])
except:
    auth = Auth.Token(env['GITHUB_TOKEN'])
    g = Github(auth=auth)

    match = r"(^Update [.\S]*$)|(^Merge branch '.*' into [\S]*$)"
    dataset = {}
    bag = []
    for r in df['repositories']:
        print(f'>> adding repo {r}')
        repo = g.get_repo(r)
        labels = [l.name for l in repo.get_labels()]
        issues_raw = repo.get_issues(state='closed')
        commits_raw = repo.get_commits()

        print('\tadding issues...')
        issues = [i for i in issues_raw if (not i.body or '```' not in i.body or i.comments == 0)]

        print('\tadding commits...')
        commits = []
        for c in commits_raw:
            msg = c.commit.message
            title = msg.split('\n')[0].strip()
            if len(title) > 0 and not re.match(match, title):
                print(f'\t\tadding commit: {title}')
                nums = re.findall(r"#(\d+)", msg)
                rel = []
                # TODO: Need to check if num is an issue or pull request (how tf do i do that)
                for n in nums:
                    rel.append(repo.get_issue(int(n)).body)
                rel = '\n'.join(rel)
                full_msg = c.commit.message + rel
                commits.append(full_msg)

        bag.extend(issues)
        bag.extend(commits)
        dataset[r] = {'issues': issues, 'commits': commits}

    g.close()
    with open('sample_data.json', 'w') as f:
        json.dump(dataset, f)

vectorizer = sk.feature_extraction.text.TfidfVectorizer(max_features=5000,
                                                        stop_words='english',
                                                        max_df=0.8,
                                                        min_df=10,
                                                        norm='l2')
doc_by_vocab = vectorizer.fit_transform(bag)

# TODO: Analyze the guys
#  issues = list of text of closed issues with at least 1 code block and 1 comment
#  commits = list of text of commits that aren't auto generated by GitHub (Ex: Update X or Merge branch 'main'...)
#   Also contains appended text of all related issues

# merge issues and commits together
for k, v in dataset.items():
    data = v['issues'] + v['commits']
